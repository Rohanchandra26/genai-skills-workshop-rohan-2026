{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Three: Testing and Evaluation\n",
    "## GenAI Delivery Excellence Skills Validation Workshop\n",
    "\n",
    "**Goal:** Demonstrate the ability to write tests and evaluate responses from large language models.\n",
    "\n",
    "### Requirements:\n",
    "1. Create functions that perform specific tasks utilizing Google Gemini\n",
    "2. Write unit tests to ensure the functions perform as expected\n",
    "3. Use the Google Evaluation API to evaluate and compare functions using various different prompts\n",
    "\n",
    "---\n",
    "### What We're Building:\n",
    "- **Function 1:** Classifies citizen questions into categories: `Employment`, `General Information`, `Emergency Services`, or `Tax Related`\n",
    "- **Function 2:** Generates social media posts for government announcements (weather emergencies, holidays, school closings, etc.)\n",
    "- **Unit Tests:** pytest-based tests for both functions\n",
    "- **Evaluation:** Google Vertex AI Evaluation Service API to assess and compare prompt strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-aiplatform google-generativeai vertexai pandas ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import unittest\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.evaluation import EvalTask\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CONFIGURE: Replace with your Google Cloud project details\n",
    "# -------------------------------------------------------\n",
    "PROJECT_ID = \"your-project-id\"       # e.g. \"my-gcp-project-12345\"\n",
    "LOCATION   = \"us-central1\"\n",
    "MODEL_ID   = \"gemini-1.5-flash-002\"  # Latest stable Gemini model\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the Gemini model (used throughout the notebook)\n",
    "model = GenerativeModel(MODEL_ID)\n",
    "\n",
    "print(f\"âœ… Vertex AI initialized â€” Project: {PROJECT_ID} | Location: {LOCATION} | Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: Gemini Functions\n",
    "\n",
    "### Function 1 â€” `classify_citizen_question(question)`\n",
    "\n",
    "This function uses Gemini to classify a citizen's question into one of four government service categories:\n",
    "- `Employment`\n",
    "- `General Information`\n",
    "- `Emergency Services`\n",
    "- `Tax Related`\n",
    "\n",
    "Because the output is constrained to a fixed set of labels, this is a **deterministic** (classification) task â€” we can test it with standard unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_citizen_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies a citizen's question into one of four government service categories\n",
    "    using Gemini.\n",
    "\n",
    "    Args:\n",
    "        question (str): The citizen's question.\n",
    "\n",
    "    Returns:\n",
    "        str: One of â€” 'Employment', 'General Information',\n",
    "             'Emergency Services', or 'Tax Related'\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "You are a government service routing assistant.\n",
    "Your ONLY job is to classify citizen questions into one of the following categories:\n",
    "\n",
    "  - Employment\n",
    "  - General Information\n",
    "  - Emergency Services\n",
    "  - Tax Related\n",
    "\n",
    "Rules:\n",
    "  1. Output ONLY the category name â€” no punctuation, no explanation.\n",
    "  2. If the question could fit multiple categories, choose the MOST specific one.\n",
    "  3. 'Emergency Services' is reserved for urgent safety/emergency topics only.\n",
    "\n",
    "Examples:\n",
    "  Question: How do I apply for unemployment benefits?\n",
    "  Category: Employment\n",
    "\n",
    "  Question: My house is on fire, who do I call?\n",
    "  Category: Emergency Services\n",
    "\n",
    "  Question: What are the office hours for City Hall?\n",
    "  Category: General Information\n",
    "\n",
    "  Question: When is my property tax due?\n",
    "  Category: Tax Related\n",
    "\n",
    "Now classify the following:\n",
    "  Question: {question}\n",
    "  Category:\"\"\".format(question=question)\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    # Strip whitespace and return the clean label\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "test_questions = [\n",
    "    \"How do I file for workers' compensation?\",\n",
    "    \"There is a gas leak on my street!\",\n",
    "    \"What documents do I need for a building permit?\",\n",
    "    \"How do I appeal my property tax assessment?\"\n",
    "]\n",
    "\n",
    "print(\"=== classify_citizen_question â€” Manual Smoke Test ===\")\n",
    "for q in test_questions:\n",
    "    result = classify_citizen_question(q)\n",
    "    print(f\"  Q: {q}\")\n",
    "    print(f\"  â†’ {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2 â€” `generate_government_social_media_post(announcement)`\n",
    "\n",
    "This function uses Gemini to generate a social media post for a government announcement.  \n",
    "Because the output is free-form text (no single correct answer), this is an **indeterminate** task.  \n",
    "We will test it using:\n",
    "1. A Gemini-based **rules checker** (unit test proxy)\n",
    "2. The **Google Evaluation API** (formal evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Prompt Version A â€” used as the baseline prompt\n",
    "# -------------------------------------------------------\n",
    "SOCIAL_MEDIA_PROMPT_V1 = \"\"\"\n",
    "You are the official social media manager for a US state government department.\n",
    "\n",
    "Your task is to write a single social media post for the following government announcement.\n",
    "\n",
    "Rules you MUST follow:\n",
    "  1. Keep the post under 280 characters (Twitter/X limit).\n",
    "  2. Use a professional but approachable tone suitable for a government account.\n",
    "  3. Include one relevant hashtag at the end (e.g., #AKGov, #AlaskaAlerts, #SchoolClosure).\n",
    "  4. Do NOT use emojis.\n",
    "  5. If it is an emergency, start with: ALERT:\n",
    "\n",
    "Announcement: {announcement}\n",
    "\n",
    "Post:\"\"\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Prompt Version B â€” improved prompt with few-shot examples\n",
    "# -------------------------------------------------------\n",
    "SOCIAL_MEDIA_PROMPT_V2 = \"\"\"\n",
    "You are the official social media manager for a US state government department.\n",
    "\n",
    "Write a social media post for the government announcement below.\n",
    "\n",
    "Rules you MUST follow:\n",
    "  1. Keep the post under 280 characters.\n",
    "  2. Professional but approachable government tone.\n",
    "  3. Include one relevant hashtag at the end.\n",
    "  4. No emojis.\n",
    "  5. Emergency announcements MUST start with: ALERT:\n",
    "\n",
    "Examples:\n",
    "  Announcement: All state offices will be closed on Thanksgiving Day, November 28.\n",
    "  Post: State offices will be closed Thursday, Nov. 28 in observance of Thanksgiving. Normal operations resume Friday, Nov. 29. #AKGov\n",
    "\n",
    "  Announcement: A major snowstorm is expected tonight. Roads may be impassable.\n",
    "  Post: ALERT: Major snowstorm expected tonight. Avoid unnecessary travel. Road conditions may be hazardous. Stay safe. #AlaskaAlerts\n",
    "\n",
    "  Announcement: All public schools in the Anchorage district will be closed tomorrow due to weather.\n",
    "  Post: Anchorage School District schools are CLOSED tomorrow due to weather. Stay safe and check back for updates. #SchoolClosure\n",
    "\n",
    "Now write a post for:\n",
    "  Announcement: {announcement}\n",
    "  Post:\"\"\"\n",
    "\n",
    "\n",
    "def generate_government_social_media_post(announcement: str, prompt_version: str = \"v1\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a social media post for a government announcement using Gemini.\n",
    "\n",
    "    Args:\n",
    "        announcement (str): The government announcement text.\n",
    "        prompt_version (str): 'v1' (basic) or 'v2' (few-shot examples).\n",
    "\n",
    "    Returns:\n",
    "        str: A social media post string.\n",
    "    \"\"\"\n",
    "    if prompt_version == \"v2\":\n",
    "        prompt_template = SOCIAL_MEDIA_PROMPT_V2\n",
    "    else:\n",
    "        prompt_template = SOCIAL_MEDIA_PROMPT_V1\n",
    "\n",
    "    prompt = prompt_template.format(announcement=announcement)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "announcements = [\n",
    "    \"All state offices will be closed on Christmas Day, December 25.\",\n",
    "    \"A severe blizzard is forecast for tomorrow. Residents should stay home.\",\n",
    "    \"All Fairbanks school district schools will be closed on Monday due to extreme cold.\"\n",
    "]\n",
    "\n",
    "print(\"=== generate_government_social_media_post â€” Prompt V1 ===\")\n",
    "for a in announcements:\n",
    "    post = generate_government_social_media_post(a, prompt_version=\"v1\")\n",
    "    print(f\"  Announcement: {a}\")\n",
    "    print(f\"  Post ({len(post)} chars): {post}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part B: Unit Tests\n",
    "\n",
    "### Testing Strategy\n",
    "\n",
    "| Function | Output Type | Testing Method |\n",
    "|---|---|---|\n",
    "| `classify_citizen_question` | Deterministic (fixed labels) | Direct `assertEqual` assertions |\n",
    "| `generate_government_social_media_post` | Indeterminate (free text) | Rules-based LLM checker + Evaluation API |\n",
    "\n",
    "We use `ipytest` to run `pytest`-style tests directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ipytest to run pytest inside the notebook\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Suite 1: `classify_citizen_question` â€” Deterministic Tests\n",
    "\n",
    "The valid categories are fixed, so we can assert exact equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Test Suite 1: Question Classification\n",
    "# Tests that classify_citizen_question returns the correct\n",
    "# category for a variety of citizen questions.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "VALID_CATEGORIES = {\"Employment\", \"General Information\", \"Emergency Services\", \"Tax Related\"}\n",
    "\n",
    "class TestClassifyCitizenQuestion:\n",
    "\n",
    "    # ---- Employment Questions ----\n",
    "\n",
    "    def test_unemployment_benefits_is_employment(self):\n",
    "        \"\"\"Questions about unemployment should be classified as Employment.\"\"\"\n",
    "        result = classify_citizen_question(\"How do I file for unemployment benefits?\")\n",
    "        assert result == \"Employment\", f\"Expected 'Employment', got '{result}'\"\n",
    "\n",
    "    def test_job_application_is_employment(self):\n",
    "        \"\"\"Questions about government job applications should be Employment.\"\"\"\n",
    "        result = classify_citizen_question(\"How do I apply for a state government job?\")\n",
    "        assert result == \"Employment\", f\"Expected 'Employment', got '{result}'\"\n",
    "\n",
    "    def test_workers_comp_is_employment(self):\n",
    "        \"\"\"Workers compensation queries should be Employment.\"\"\"\n",
    "        result = classify_citizen_question(\"How do I file a workers' compensation claim?\")\n",
    "        assert result == \"Employment\", f\"Expected 'Employment', got '{result}'\"\n",
    "\n",
    "    # ---- Emergency Services Questions ----\n",
    "\n",
    "    def test_gas_leak_is_emergency(self):\n",
    "        \"\"\"Gas leak reports should be Emergency Services.\"\"\"\n",
    "        result = classify_citizen_question(\"There is a gas leak on my street, who do I call?\")\n",
    "        assert result == \"Emergency Services\", f\"Expected 'Emergency Services', got '{result}'\"\n",
    "\n",
    "    def test_house_fire_is_emergency(self):\n",
    "        \"\"\"Fire emergencies should be Emergency Services.\"\"\"\n",
    "        result = classify_citizen_question(\"My neighbor's house is on fire!\")\n",
    "        assert result == \"Emergency Services\", f\"Expected 'Emergency Services', got '{result}'\"\n",
    "\n",
    "    # ---- General Information Questions ----\n",
    "\n",
    "    def test_office_hours_is_general_info(self):\n",
    "        \"\"\"Questions about office hours are General Information.\"\"\"\n",
    "        result = classify_citizen_question(\"What are the hours of operation for the DMV?\")\n",
    "        assert result == \"General Information\", f\"Expected 'General Information', got '{result}'\"\n",
    "\n",
    "    def test_building_permit_is_general_info(self):\n",
    "        \"\"\"Permit document questions are General Information.\"\"\"\n",
    "        result = classify_citizen_question(\"What documents do I need to get a building permit?\")\n",
    "        assert result == \"General Information\", f\"Expected 'General Information', got '{result}'\"\n",
    "\n",
    "    # ---- Tax Related Questions ----\n",
    "\n",
    "    def test_property_tax_is_tax_related(self):\n",
    "        \"\"\"Property tax questions should be Tax Related.\"\"\"\n",
    "        result = classify_citizen_question(\"When is my property tax payment due?\")\n",
    "        assert result == \"Tax Related\", f\"Expected 'Tax Related', got '{result}'\"\n",
    "\n",
    "    def test_tax_appeal_is_tax_related(self):\n",
    "        \"\"\"Tax assessment appeal questions should be Tax Related.\"\"\"\n",
    "        result = classify_citizen_question(\"How do I appeal my property tax assessment?\")\n",
    "        assert result == \"Tax Related\", f\"Expected 'Tax Related', got '{result}'\"\n",
    "\n",
    "    # ---- Output Validity Check ----\n",
    "\n",
    "    def test_output_is_always_valid_category(self):\n",
    "        \"\"\"Function must ALWAYS return one of the four valid categories.\"\"\"\n",
    "        test_inputs = [\n",
    "            \"Where do I pay my water bill?\",\n",
    "            \"Is the park open on weekends?\",\n",
    "            \"Can I get a job at the city?\",\n",
    "            \"I smell gas outside my window.\"\n",
    "        ]\n",
    "        for q in test_inputs:\n",
    "            result = classify_citizen_question(q)\n",
    "            assert result in VALID_CATEGORIES, (\n",
    "                f\"Invalid category '{result}' returned for question: '{q}'\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Rules Checker Helper\n",
    "\n",
    "Since `generate_government_social_media_post` produces free-form text, we use a second Gemini call to evaluate whether the generated post follows the rules. This is a standard pattern for testing indeterminate LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_post_follow_rules(post: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses Gemini as a judge to check whether a social media post follows\n",
    "    the government posting rules.\n",
    "\n",
    "    Rules checked:\n",
    "      1. Under 280 characters\n",
    "      2. Professional tone\n",
    "      3. Contains exactly one hashtag\n",
    "      4. No emojis\n",
    "\n",
    "    Returns:\n",
    "        'Yes' if the post follows all rules, 'No' otherwise.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "Does the following social media post follow ALL of these rules?\n",
    "  1. It is under 280 characters.\n",
    "  2. It uses a professional, government-appropriate tone (no slang).\n",
    "  3. It contains at least one hashtag.\n",
    "  4. It contains NO emojis.\n",
    "\n",
    "Answer ONLY 'Yes' or 'No'. Do not explain.\n",
    "\n",
    "Post: {post}\n",
    "Answer:\"\"\".format(post=post)\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "def does_emergency_post_start_with_alert(post: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks specifically whether an emergency post starts with 'ALERT:'.\n",
    "\n",
    "    Returns:\n",
    "        'Yes' if the post starts with 'ALERT:', 'No' otherwise.\n",
    "    \"\"\"\n",
    "    # This is deterministic â€” we can check it directly without an LLM call\n",
    "    return \"Yes\" if post.upper().startswith(\"ALERT:\") else \"No\"\n",
    "\n",
    "\n",
    "# Quick validation of the rules checker itself\n",
    "good_post = \"State offices closed Dec. 25 for Christmas. Happy holidays from all of us. #AKGov\"\n",
    "bad_post  = \"Hey everyone!! ðŸŽ„ Offices r closed lol no hashtag here\"\n",
    "\n",
    "print(f\"Good post follows rules: {does_post_follow_rules(good_post)}\")   # Expected: Yes\n",
    "print(f\"Bad post follows rules:  {does_post_follow_rules(bad_post)}\")    # Expected: No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Suite 2: `generate_government_social_media_post` â€” LLM-Judged Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Test Suite 2: Social Media Post Generation\n",
    "# Uses an LLM judge to verify rule compliance, since output\n",
    "# is indeterminate (no single correct answer).\n",
    "# -------------------------------------------------------\n",
    "\n",
    "class TestGenerateGovernmentSocialMediaPost:\n",
    "\n",
    "    # ---- Rule Compliance Tests ----\n",
    "\n",
    "    def test_holiday_post_v1_follows_rules(self):\n",
    "        \"\"\"Holiday posts (v1 prompt) should follow all government posting rules.\"\"\"\n",
    "        post = generate_government_social_media_post(\n",
    "            \"All state offices will be closed on Christmas Day, December 25.\",\n",
    "            prompt_version=\"v1\"\n",
    "        )\n",
    "        result = does_post_follow_rules(post)\n",
    "        assert result == \"Yes\", f\"Post violated rules.\\nPost: {post}\\nJudge: {result}\"\n",
    "\n",
    "    def test_school_closure_post_v1_follows_rules(self):\n",
    "        \"\"\"School closure posts (v1 prompt) should follow all rules.\"\"\"\n",
    "        post = generate_government_social_media_post(\n",
    "            \"All Anchorage school district schools will be closed Monday due to extreme cold.\",\n",
    "            prompt_version=\"v1\"\n",
    "        )\n",
    "        result = does_post_follow_rules(post)\n",
    "        assert result == \"Yes\", f\"Post violated rules.\\nPost: {post}\\nJudge: {result}\"\n",
    "\n",
    "    def test_holiday_post_v2_follows_rules(self):\n",
    "        \"\"\"Holiday posts (v2 prompt) should follow all government posting rules.\"\"\"\n",
    "        post = generate_government_social_media_post(\n",
    "            \"All state offices will be closed on New Year's Day, January 1.\",\n",
    "            prompt_version=\"v2\"\n",
    "        )\n",
    "        result = does_post_follow_rules(post)\n",
    "        assert result == \"Yes\", f\"Post violated rules.\\nPost: {post}\\nJudge: {result}\"\n",
    "\n",
    "    # ---- Emergency Post Test ----\n",
    "\n",
    "    def test_emergency_post_starts_with_alert(self):\n",
    "        \"\"\"Emergency posts MUST start with 'ALERT:' per the posting rules.\"\"\"\n",
    "        post = generate_government_social_media_post(\n",
    "            \"A severe blizzard warning has been issued. Roads will be impassable tonight.\",\n",
    "            prompt_version=\"v2\"\n",
    "        )\n",
    "        result = does_emergency_post_start_with_alert(post)\n",
    "        assert result == \"Yes\", f\"Emergency post did not start with ALERT:\\nPost: {post}\"\n",
    "\n",
    "    # ---- Character Limit Test ----\n",
    "\n",
    "    def test_post_under_280_characters(self):\n",
    "        \"\"\"All generated posts must be under 280 characters.\"\"\"\n",
    "        post = generate_government_social_media_post(\n",
    "            \"The annual state fair will be held August 21-31 at the state fairgrounds. Admission is $12.\",\n",
    "            prompt_version=\"v1\"\n",
    "        )\n",
    "        assert len(post) <= 280, f\"Post exceeds 280 characters ({len(post)} chars):\\n{post}\"\n",
    "\n",
    "    # ---- Negative Test: Deliberately bad post ----\n",
    "\n",
    "    def test_hardcoded_bad_post_fails_rules_check(self):\n",
    "        \"\"\"\n",
    "        Validates that the rules checker correctly identifies\n",
    "        a manually crafted bad post as rule-violating.\n",
    "        This is a meta-test to ensure our judge is working.\n",
    "        \"\"\"\n",
    "        bad_post = (\n",
    "            \"ðŸŽ‰ðŸŽ„ Hey guys!! Offices r closed tomorrow lol enjoy ur day off!! \"\n",
    "            \"This message is definitely way too long and informal for a government account and has NO hashtag at all \"\n",
    "            \"and also contains multiple emojis which is against the rules we set.\"\n",
    "        )\n",
    "        result = does_post_follow_rules(bad_post)\n",
    "        assert result == \"No\", f\"Expected rule checker to return 'No' for bad post, got '{result}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part C: Google Evaluation API\n",
    "\n",
    "### Why Use the Evaluation API?\n",
    "\n",
    "Unit tests verify rule compliance (pass/fail), but they don't measure **quality**.  \n",
    "The Vertex AI Evaluation Service measures quality using model-based metrics:\n",
    "\n",
    "| Metric | What It Measures |\n",
    "|---|---|\n",
    "| `fluency` | Natural, grammatically correct language (1â€“5) |\n",
    "| `coherence` | Logical, easy-to-follow structure (1â€“5) |\n",
    "| `fulfillment` | Does the response completely fulfill the instructions? (1â€“5) |\n",
    "| `groundedness` | Does the response stick to the provided context? (0/1) |\n",
    "\n",
    "We use the Evaluation API to:\n",
    "1. Evaluate the **social media post generator** using both prompt versions (V1 vs V2)\n",
    "2. Compare results to determine which prompt produces better quality output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C1: Create the Evaluation Dataset\n",
    "\n",
    "We create a dataset of government announcements and generate responses from both prompt versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Evaluation Dataset: Government Announcements\n",
    "# -------------------------------------------------------\n",
    "\n",
    "announcements_dataset = [\n",
    "    \"All state government offices will be closed on Thanksgiving Day, November 28th.\",\n",
    "    \"A severe winter storm warning is in effect. Residents should avoid travel tonight and tomorrow.\",\n",
    "    \"All public schools in the Fairbanks North Star Borough will be closed Monday due to temperatures below -40Â°F.\",\n",
    "    \"The Department of Motor Vehicles will be closed for system maintenance this Saturday, December 7.\",\n",
    "    \"A boil water advisory has been issued for residents in the Eagle River area due to a water main break.\",\n",
    "    \"The annual property tax payment deadline is December 31st. Payments can be made online or at City Hall.\",\n",
    "    \"State parks will offer free admission on the first Saturday of every month through April.\",\n",
    "    \"A wildfire is approaching the Palmer area. Residents in zones A and B are ordered to evacuate immediately.\"\n",
    "]\n",
    "\n",
    "# Generate responses using both prompt versions\n",
    "print(\"Generating responses for evaluation dataset...\")\n",
    "responses_v1 = []\n",
    "responses_v2 = []\n",
    "\n",
    "for i, announcement in enumerate(announcements_dataset):\n",
    "    print(f\"  Processing announcement {i+1}/{len(announcements_dataset)}...\", end=\"\")\n",
    "    r_v1 = generate_government_social_media_post(announcement, prompt_version=\"v1\")\n",
    "    r_v2 = generate_government_social_media_post(announcement, prompt_version=\"v2\")\n",
    "    responses_v1.append(r_v1)\n",
    "    responses_v2.append(r_v2)\n",
    "    print(\" done\")\n",
    "\n",
    "print(\"\\nâœ… Response generation complete.\")\n",
    "\n",
    "# Preview the responses side by side\n",
    "df_preview = pd.DataFrame({\n",
    "    \"Announcement\": [a[:60] + \"...\" for a in announcements_dataset],\n",
    "    \"V1 Response\": [r[:80] + \"...\" for r in responses_v1],\n",
    "    \"V2 Response\": [r[:80] + \"...\" for r in responses_v2],\n",
    "})\n",
    "print(\"\\n=== Response Preview ===\")\n",
    "print(df_preview.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C2: Build Evaluation DataFrames\n",
    "\n",
    "The Evaluation API requires a pandas DataFrame with specific column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Build Eval DataFrames for Prompt V1 and V2\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# The full prompt (instruction + announcement) becomes the 'content' and 'instruction'\n",
    "full_prompts_v1 = [SOCIAL_MEDIA_PROMPT_V1.format(announcement=a) for a in announcements_dataset]\n",
    "full_prompts_v2 = [SOCIAL_MEDIA_PROMPT_V2.format(announcement=a) for a in announcements_dataset]\n",
    "\n",
    "# The announcement alone serves as 'context' (grounding reference)\n",
    "contexts = announcements_dataset\n",
    "\n",
    "# Evaluation DataSet â€” Prompt V1\n",
    "eval_dataset_v1 = pd.DataFrame({\n",
    "    \"content\":    full_prompts_v1,\n",
    "    \"instruction\": full_prompts_v1,\n",
    "    \"context\":    contexts,\n",
    "    \"response\":   responses_v1,    # Pre-computed responses (avoids re-running model in eval)\n",
    "})\n",
    "\n",
    "# Evaluation DataSet â€” Prompt V2\n",
    "eval_dataset_v2 = pd.DataFrame({\n",
    "    \"content\":    full_prompts_v2,\n",
    "    \"instruction\": full_prompts_v2,\n",
    "    \"context\":    contexts,\n",
    "    \"response\":   responses_v2,\n",
    "})\n",
    "\n",
    "print(f\"âœ… Eval dataset V1: {len(eval_dataset_v1)} rows\")\n",
    "print(f\"âœ… Eval dataset V2: {len(eval_dataset_v2)} rows\")\n",
    "print(\"\\nDataset V1 columns:\", list(eval_dataset_v1.columns))\n",
    "eval_dataset_v1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C3: Create EvalTask and Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Metrics to evaluate\n",
    "# -------------------------------------------------------\n",
    "EVAL_METRICS = [\"fluency\", \"coherence\", \"fulfillment\", \"groundedness\"]\n",
    "\n",
    "# Unique experiment name (Vertex AI Experiments must be globally unique per project)\n",
    "EXPERIMENT_NAME = \"govt-social-media-post-eval\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# EvalTask for Prompt V1\n",
    "# -------------------------------------------------------\n",
    "eval_task_v1 = EvalTask(\n",
    "    dataset=eval_dataset_v1,\n",
    "    metrics=EVAL_METRICS,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# EvalTask for Prompt V2\n",
    "# -------------------------------------------------------\n",
    "eval_task_v2 = EvalTask(\n",
    "    dataset=eval_dataset_v2,\n",
    "    metrics=EVAL_METRICS,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "print(\"âœ… EvalTask objects created for V1 and V2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Run Evaluation â€” Prompt V1\n",
    "# -------------------------------------------------------\n",
    "run_ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(\"Running evaluation for Prompt V1 (basic prompt)...\")\n",
    "result_v1 = eval_task_v1.evaluate(\n",
    "    model=model,\n",
    "    experiment_run_name=f\"social-media-v1-{run_ts}\"\n",
    ")\n",
    "print(\"âœ… V1 evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Run Evaluation â€” Prompt V2\n",
    "# -------------------------------------------------------\n",
    "run_ts_v2 = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(\"Running evaluation for Prompt V2 (few-shot prompt)...\")\n",
    "result_v2 = eval_task_v2.evaluate(\n",
    "    model=model,\n",
    "    experiment_run_name=f\"social-media-v2-{run_ts_v2}\"\n",
    ")\n",
    "print(\"âœ… V2 evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C4: Display and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_eval_report(label: str, result) -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints evaluation summary metrics and the per-row metrics table.\n",
    "\n",
    "    Args:\n",
    "        label (str): A human-readable label for the evaluation run.\n",
    "        result: The EvalResult object returned by EvalTask.evaluate().\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  EVALUATION REPORT: {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nðŸ“Š Summary Metrics:\")\n",
    "    print(result.summary_metrics.to_string())\n",
    "    print(\"\\nðŸ“‹ Per-Row Metrics Table (first 5 rows):\")\n",
    "    print(result.metrics_table.head(5).to_string())\n",
    "    print()\n",
    "\n",
    "\n",
    "display_eval_report(\"Prompt V1 (Basic)\", result_v1)\n",
    "display_eval_report(\"Prompt V2 (Few-Shot Examples)\", result_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C5: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Head-to-Head Comparison: V1 vs V2\n",
    "# -------------------------------------------------------\n",
    "\n",
    "metrics_to_compare = [\"fluency/mean\", \"coherence/mean\", \"fulfillment/mean\", \"groundedness/mean\"]\n",
    "\n",
    "comparison_data = {}\n",
    "for metric in metrics_to_compare:\n",
    "    v1_val = result_v1.summary_metrics.get(metric, \"N/A\")\n",
    "    v2_val = result_v2.summary_metrics.get(metric, \"N/A\")\n",
    "    comparison_data[metric] = {\n",
    "        \"Prompt V1 (Basic)\": round(v1_val, 3) if isinstance(v1_val, float) else v1_val,\n",
    "        \"Prompt V2 (Few-Shot)\": round(v2_val, 3) if isinstance(v2_val, float) else v2_val,\n",
    "        \"Winner\": \"V2\" if (isinstance(v2_val, float) and isinstance(v1_val, float) and v2_val > v1_val)\n",
    "                  else (\"V1\" if (isinstance(v1_val, float) and isinstance(v2_val, float) and v1_val > v2_val)\n",
    "                        else \"Tie\")\n",
    "    }\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PROMPT V1 vs V2 â€” Head-to-Head Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(df_comparison.to_string())\n",
    "print()\n",
    "\n",
    "# Count wins\n",
    "v2_wins = (df_comparison[\"Winner\"] == \"V2\").sum()\n",
    "v1_wins = (df_comparison[\"Winner\"] == \"V1\").sum()\n",
    "\n",
    "print(f\"\\nðŸ† Overall: V1 wins {v1_wins} metric(s), V2 wins {v2_wins} metric(s)\")\n",
    "if v2_wins > v1_wins:\n",
    "    print(\"âœ… Recommendation: Use Prompt V2 (few-shot examples) for better quality social media posts.\")\n",
    "elif v1_wins > v2_wins:\n",
    "    print(\"âœ… Recommendation: Prompt V1 (basic) performs comparably â€” keep it simple.\")\n",
    "else:\n",
    "    print(\"âœ… Recommendation: Both prompts perform similarly. V2 provides more predictable output formatting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What Was Built\n",
    "\n",
    "| Component | Description |\n",
    "|---|---|\n",
    "| `classify_citizen_question()` | Gemini-powered question classifier (4 categories) |\n",
    "| `generate_government_social_media_post()` | Gemini-powered social media post generator (2 prompt versions) |\n",
    "| `does_post_follow_rules()` | LLM-based rules checker for indeterminate outputs |\n",
    "| `TestClassifyCitizenQuestion` | 9 pytest unit tests for classification (deterministic) |\n",
    "| `TestGenerateGovernmentSocialMediaPost` | 5 pytest tests using LLM judge (indeterminate) |\n",
    "| Google Evaluation API | Evaluated V1 vs V2 on fluency, coherence, fulfillment, groundedness |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Deterministic LLM functions** (classification, sentiment analysis) can be tested like any regular function â€” assert the exact output.\n",
    "2. **Indeterminate LLM functions** (text generation) require an LLM-as-judge approach for unit testing.\n",
    "3. **The Google Evaluation API** provides model-based quality metrics that go beyond pass/fail to measure real-world output quality.\n",
    "4. **Few-shot prompting** (V2) typically improves quality metrics by demonstrating the expected output format to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
